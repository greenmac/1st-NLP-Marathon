{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本日課程-文字預處理，部分內容前面章節可能提過，這裡會將前處理所需技巧串起"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from nltk.corpus import stopwords\n",
    "#tsv是指用tab分開字元的檔案\n",
    "dataset=pd.read_csv('../data/Restaurant_Reviews.tsv',delimiter='\\t',quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "review before preprocessing : Wow... Loved this place.\n"
     ]
    }
   ],
   "source": [
    "print('review before preprocessing : {}'.format(dataset['Review'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 運用re.sub去除部分字元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "#re.sub用來去除不要字元，第一個參數是要去除字元，但可以透過添加＾，變成不要去除字元\n",
    "#第二個參數是去除字元後這些東西要變成什麼，在這我們是希望它變成一個空格\n",
    "#第三個參數則是我們要剝除的字元從哪裡來\n",
    "\n",
    "review=re.sub('[^a-zA-Z]',' ',dataset['Review'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "review after re.sub : Wow    Loved this place \n"
     ]
    }
   ],
   "source": [
    "print('review after re.sub : {}'.format(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 將所有字母轉為小寫:因為大部分情境區分大小寫並不能提供而外訊息，如CV內顏色無法提供額外訊息時我們會將圖像轉為灰階，藉此降低複雜度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "review after lower : wow    loved this place \n"
     ]
    }
   ],
   "source": [
    "#把全部變成小寫\n",
    "review=review.lower()\n",
    "print('review after lower : {}'.format(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 斷詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "review after split : ['wow', 'loved', 'this', 'place']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#把review裡面的單字切開\n",
    "print('review after split : {}'.format(review.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* tokenize 相較於split會是更好的選擇，如 split 無法分開 word. 這種case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Wow', '...', 'Loved', 'this', 'place', '.']"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "nltk.word_tokenize('Wow... Loved this place.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "review after tokenize : ['wow', 'loved', 'this', 'place']\n"
     ]
    }
   ],
   "source": [
    "review = nltk.word_tokenize(review)\n",
    "print('review after tokenize : {}'.format(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 中文使用 jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "jieba.set_dictionary('../data/dict.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Building prefix dict from e:\\python\\1st-NLP-Marathon-CUPOY\\1st-NLP-Marathon\\data\\dict.txt ...\n",
      "Dumping model to file cache C:\\Users\\alway\\AppData\\Local\\Temp\\jieba.u15223495c9b0a56c7b1e9ab9d5c41801.cache\n",
      "Loading model cost 0.616 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "output: 哇|！|我|好|喜歡|這|個|地方\n"
     ]
    }
   ],
   "source": [
    "review_ = '哇！我好喜歡這個地方'\n",
    "cut_result = jieba.cut(review_, cut_all=False, HMM=False)\n",
    "print(\"output: {}\".format('|'.join(cut_result)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stopwords: 移除贅字，此步驟為前處理的重要步驟之一，過多的贅字不僅無法提供更多訊息，還會干擾到模型的訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alway\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "#處理文字，有建立好的文字褲會幫我們移除不想要的文字\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "review after removeing stopwords : ['wow', 'loved', 'place']\n"
     ]
    }
   ],
   "source": [
    "review=[word for word in review if not word in set(stopwords.words('english'))]\n",
    "print('review after removeing stopwords : {}'.format(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* stopwords.words('english') 是一個建立好的list，包含一些常見的英文贅字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we']"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "stopwords.words('english')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 我們也可以自己建立 stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source:https://github.com/tomlinNTUB/Machine-Learning\n",
    "with open('停用詞-繁體中文.txt','r') as file:\n",
    "    stop_words = file.readlines()\n",
    "stop_words = [word.strip('\\n') for word in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "practice_sentence after removeing stopwords : ['現在', '好想', '睡覺']\n"
     ]
    }
   ],
   "source": [
    "practice_sentence = ['哈哈','!','現在','好想','睡覺','啊']\n",
    "practice_sentence=[word for word in practice_sentence if not word in set(stop_words)]\n",
    "print('practice_sentence after removeing stopwords : {}'.format(practice_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming: 詞幹提取\n",
    " * ex. loves,loved都變成love\n",
    " * 中文沒有詞幹提取的需求"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "review=[ps.stem(word) for word in review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review after stemming : ['wow', 'love', 'place']\n"
     ]
    }
   ],
   "source": [
    "print('review after stemming : {}'.format(review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 練習清理所有的句子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset=pd.read_csv('movie_feedback.csv',encoding = 'Big5',names=['feedback', 'label'] )\n",
    "dataset=pd.read_csv('Restaurant_Reviews.tsv',delimiter='\\t',quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "row=len(dataset)\n",
    "for i in range(0,row):\n",
    "    review=re.sub('[^a-zA-Z]',' ',dataset['Review'][i])\n",
    "    review=review.lower()\n",
    "    review=review.split()\n",
    "    ps=PorterStemmer()\n",
    "    ## 這裡先不用stopwords 因為 review中很多反定詞會被移掉 如isn't good, 會變成 good\n",
    "    review=[ps.stem(word) for word in review ]\n",
    "    review=' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 手動選出現頻率較高的單字：一般來說我們不需要自己處理這個步驟，通常文字轉向量或index的api都有參數可以設定，這裡是讓大家自己練習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 從整個corpus中取出所有的單詞\n",
    "whole_words = []\n",
    "for sentence in corpus:\n",
    "    for words in nltk.word_tokenize(sentence):\n",
    "        whole_words.append(words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 取出出現頻率top_k的單詞\n",
    "top_k = 1000\n",
    "top_k_words = []\n",
    "for item in Counter(whole_words).most_common(top_k):\n",
    "    top_k_words.append(item[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以 corpus中第一個句子為範例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_low_frequency_word=' '.join([word for word in nltk.word_tokenize(corpus[0]) if word in set(top_k_words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing low frequency words:\n",
      " wow love thi place\n",
      "\n",
      "\n",
      "After removing low frequency words:\n",
      " wow love thi place\n"
     ]
    }
   ],
   "source": [
    "print('Before removing low frequency words:\\n {}'.format(corpus[0]))\n",
    "print('\\n')\n",
    "print('After removing low frequency words:\\n {}'.format(remove_low_frequency_word))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 轉bag-of-words vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#Creating bag of word model\n",
    "#tokenization(符號化)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#max_features是要建造幾個column，會按造字出現的高低去篩選 \n",
    "cv=CountVectorizer(max_features=1000)\n",
    "#toarray是建造matrixs\n",
    "#X現在為sparsity就是很多零的matrix\n",
    "X=cv.fit_transform(corpus).toarray()\n",
    "y=dataset.iloc[:,1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 選擇練習: 將處理好數據放入 naive_bayes模型，並預測評論為正向或負面，詳細原理之後章節會解釋。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "\n",
    "# Feature Scaling\n",
    "\n",
    "#Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "message='I really like this!!'\n",
    "## 要使用一樣的前處理\n",
    "review=re.sub('[^a-zA-Z]',' ',message)\n",
    "review=review.lower()\n",
    "review=review.split()\n",
    "ps=PorterStemmer()\n",
    "review=[ps.stem(word) for word in review]\n",
    "review = ' '.join(review)\n",
    "input_ = cv.transform([review]).toarray()\n",
    "prediction = classifier.predict(input_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction ## 1代表正向評價"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "message='All dishes are disgusting !!'\n",
    "review=re.sub('[^a-zA-Z]',' ',message)\n",
    "review=review.lower()\n",
    "review=review.split()\n",
    "ps=PorterStemmer()\n",
    "review=[ps.stem(word) for word in review]\n",
    "review = ' '.join(review)\n",
    "input_ = cv.transform([review]).toarray()\n",
    "prediction = classifier.predict(input_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction ## 0代表負面評價"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "b071336fb10f883fe7257e8e92e9c59e797cbbef23e0f498fe46632596d585c7"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}